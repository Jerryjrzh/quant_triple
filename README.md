# Stock Analysis System

[![Docker Setup](https://img.shields.io/badge/Docker-Ready-blue?logo=docker)](DOCKER_SETUP_SUMMARY.md)
[![API Status](https://img.shields.io/badge/API-Working-green?logo=fastapi)](http://localhost:8000/docs)
[![Database](https://img.shields.io/badge/Database-PostgreSQL-blue?logo=postgresql)](docker-compose.yml)
[![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)](requirements.txt)

A comprehensive stock analysis system that leverages calendar-based temporal analysis as its core foundation, integrating opportunity identification, risk assessment, stock screening strategies, and review marking capabilities.

> **ğŸš€ å¿«é€Ÿå¼€å§‹**: ä½¿ç”¨ `sudo docker-compose up -d postgres redis && python start_server.py` ä¸€é”®å¯åŠ¨ç³»ç»Ÿ

## ğŸ“Š Current System Status

**Phase 1 (Foundation & Core Infrastructure)**: ğŸŸ¢ **75% Complete**

### âœ… Fully Implemented Components
- **Spring Festival Analysis Engine**: Complete temporal alignment and pattern recognition
- **Multi-Source Data Manager**: Tushare, AkShare, and local TDX data integration with failover
- **Data Quality Engine**: ML-based validation with Isolation Forest anomaly detection
- **ETL Pipeline**: Celery-powered background processing with comprehensive error handling
- **Database Infrastructure**: PostgreSQL with Alembic migrations and connection pooling
- **API Foundation**: FastAPI with JWT authentication, rate limiting, and async support
- **Configuration System**: Environment-based configuration with validation

### ğŸ”„ In Progress
- **Parallel Processing**: Dask integration for large-scale analysis
- **Web Visualization**: Interactive Spring Festival charts and dashboards
- **Advanced Analytics**: Risk management and institutional tracking engines

## ğŸŒŸ Key Features

### âœ… Spring Festival Alignment Analysis (Implemented)
- **Temporal Anchor Analysis**: Uses Chinese New Year as temporal anchor points to normalize and compare historical stock performance patterns
- **Seasonal Pattern Recognition**: ML-powered pattern identification with confidence scoring and consistency analysis
- **Multi-Year Data Alignment**: Sophisticated algorithms for aligning stock data across multiple years relative to Spring Festival dates
- **Trading Signal Generation**: Automated signal generation based on historical seasonal patterns

### âœ… Advanced Data Management (Implemented)
- **Multi-Source Data Integration**: Seamless integration with Tushare, AkShare, and local TDX data sources
- **Intelligent Failover**: Circuit breaker pattern with automatic source switching for maximum reliability
- **ML-Based Data Quality**: Isolation Forest anomaly detection with comprehensive quality scoring
- **ETL Pipeline**: Celery-powered background processing with error handling and retry mechanisms

### âœ… Production-Ready Infrastructure (Implemented)
- **FastAPI Backend**: High-performance async API with JWT authentication and rate limiting
- **PostgreSQL Database**: Robust data storage with Alembic migrations and connection pooling
- **Redis Caching**: Performance optimization with intelligent caching strategies
- **Docker Support**: Complete containerization for easy deployment

### ğŸš§ Planned Features (In Development)
- **Interactive Visualizations**: WebGL-accelerated charts with Spring Festival overlays
- **Institutional Fund Tracking**: Dragon-tiger list analysis and attention scoring
- **Advanced Risk Management**: Dynamic VaR calculation and seasonal risk assessment
- **Multi-Dimensional Screening**: Technical, seasonal, and institutional factor screening

## ğŸ—ï¸ Architecture

The system follows a four-layer architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Presentation Layer                          â”‚
â”‚  React UI â€¢ Interactive Charts â€¢ Real-time Dashboards      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 Application Layer                           â”‚
â”‚  FastAPI â€¢ Stock Pool Manager â€¢ Alert Engine â€¢ API Gateway â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 Analysis Layer                              â”‚
â”‚  Spring Festival Engine â€¢ Risk Engine â€¢ ML Models â€¢ Pluginsâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 Data Layer                                  â”‚
â”‚  PostgreSQL â€¢ Redis Cache â€¢ ETL Pipeline â€¢ Data Sources    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç³»ç»Ÿè¦æ±‚

- Python 3.9+
- Docker & Docker Compose (æ¨è)
- æˆ–è€… PostgreSQL 12+ å’Œ Redis 6+ (æœ¬åœ°å®‰è£…)

### å®‰è£…æ–¹å¼

#### æ–¹å¼ä¸€ï¼šDocker å®‰è£… (æ¨è)

è¿™æ˜¯æœ€ç®€å•ã€æœ€å¯é çš„å®‰è£…æ–¹å¼ï¼Œè‡ªåŠ¨å¤„ç†æ‰€æœ‰ä¾èµ–ã€‚

1. **å…‹éš†é¡¹ç›®**
   ```bash
   git clone https://github.com/your-org/stock-analysis-system.git
   cd stock-analysis-system
   ```

2. **å¯åŠ¨ Docker æœåŠ¡**
   ```bash
   # å¯åŠ¨ PostgreSQL å’Œ Redis å®¹å™¨
   sudo docker-compose up -d postgres redis
   
   # éªŒè¯æœåŠ¡çŠ¶æ€ (åº”è¯¥æ˜¾ç¤º healthy)
   sudo docker-compose ps
   ```

3. **è®¾ç½® Python ç¯å¢ƒ**
   ```bash
   # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   
   # å®‰è£…ä¾èµ–
   pip install -r requirements.txt
   ```

4. **é…ç½®ç¯å¢ƒå˜é‡**
   ```bash
   # å¤åˆ¶é…ç½®æ–‡ä»¶
   cp .env.example .env
   
   # ç¡®ä¿æ•°æ®åº“å¯†ç æ­£ç¡® (é»˜è®¤: password)
   # å¯ä»¥ç›´æ¥ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œæˆ–æ ¹æ®éœ€è¦ä¿®æ”¹
   ```

5. **åˆå§‹åŒ–æ•°æ®åº“**
   ```bash
   # è¿è¡Œæ•°æ®åº“è¿ç§»
   make db-upgrade
   
   # æˆ–è€…ä½¿ç”¨å®Œæ•´å‘½ä»¤
   DB_HOST=localhost DB_PORT=5432 DB_NAME=stock_analysis DB_USER=postgres DB_PASSWORD=password alembic upgrade head
   ```

6. **å¯åŠ¨åº”ç”¨ç¨‹åº**
   ```bash
   # æ–¹å¼1: ä½¿ç”¨æ™ºèƒ½å¯åŠ¨è„šæœ¬ (æ¨è)
   python start_server.py
   
   # æ–¹å¼2: ä½¿ç”¨ Make å‘½ä»¤
   make run-dev
   
   # æ–¹å¼3: ä½¿ç”¨ Docker ç®¡ç†å‘½ä»¤
   make docker-up    # å¯åŠ¨ Docker æœåŠ¡
   make start-server # å¯åŠ¨ API æœåŠ¡å™¨
   ```

7. **éªŒè¯å®‰è£…**
   ```bash
   # æµ‹è¯• API ç«¯ç‚¹
   python test_api.py
   
   # æˆ–ä½¿ç”¨ Make å‘½ä»¤
   make test-api
   
   # æˆ–æ‰‹åŠ¨æµ‹è¯•
   curl http://localhost:8000/health
   ```

**ğŸ‰ æˆåŠŸï¼** ç³»ç»Ÿç°åœ¨è¿è¡Œåœ¨ http://localhost:8000

### å¯ç”¨çš„ Make å‘½ä»¤

```bash
# Docker æœåŠ¡ç®¡ç†
make docker-up       # å¯åŠ¨ PostgreSQL å’Œ Redis
make docker-down     # åœæ­¢æ‰€æœ‰ Docker æœåŠ¡
make docker-status   # æŸ¥çœ‹ Docker æœåŠ¡çŠ¶æ€

# åº”ç”¨ç¨‹åºç®¡ç†
make start-server    # å¯åŠ¨ API æœåŠ¡å™¨ (æ™ºèƒ½è„šæœ¬)
make run-dev         # å¯åŠ¨å¼€å‘æœåŠ¡å™¨
make test-api        # æµ‹è¯• API ç«¯ç‚¹

# æ•°æ®åº“ç®¡ç†
make db-upgrade      # è¿è¡Œæ•°æ®åº“è¿ç§»
make db-downgrade    # å›æ»šæ•°æ®åº“è¿ç§»

# å¼€å‘å·¥å…·
make test           # è¿è¡Œæµ‹è¯•å¥—ä»¶
make lint           # ä»£ç è´¨é‡æ£€æŸ¥
make format         # ä»£ç æ ¼å¼åŒ–
```

### æ•…éšœæ’é™¤

#### Docker æƒé™é—®é¢˜
å¦‚æœé‡åˆ°æƒé™é”™è¯¯ï¼š
```bash
# æ–¹æ³•1: ä½¿ç”¨ sudo (ä¸´æ—¶è§£å†³)
sudo docker-compose up -d postgres redis

# æ–¹æ³•2: æ·»åŠ ç”¨æˆ·åˆ° docker ç»„ (æ°¸ä¹…è§£å†³)
sudo usermod -aG docker $USER
# ç„¶åæ³¨é”€é‡æ–°ç™»å½•ï¼Œæˆ–è¿è¡Œ: newgrp docker
```

#### æ•°æ®åº“è¿æ¥é—®é¢˜
```bash
# æ£€æŸ¥ Docker å®¹å™¨çŠ¶æ€
sudo docker-compose ps

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
sudo docker-compose logs postgres

# ç¡®ä¿ .env æ–‡ä»¶ä¸­çš„å¯†ç åŒ¹é… docker-compose.yml
# é»˜è®¤å¯†ç : password
```

#### ç«¯å£å†²çª
å¦‚æœç«¯å£ 5432 æˆ– 6379 è¢«å ç”¨ï¼š
```bash
# æ£€æŸ¥ç«¯å£ä½¿ç”¨æƒ…å†µ
sudo netstat -tlnp | grep :5432
sudo netstat -tlnp | grep :6379

# ä¿®æ”¹ docker-compose.yml ä¸­çš„ç«¯å£æ˜ å°„
# ä¾‹å¦‚: "15432:5432" å’Œ "16379:6379"
```

#### API æœåŠ¡å™¨é—®é¢˜
```bash
# ä½¿ç”¨æ™ºèƒ½å¯åŠ¨è„šæœ¬è·å¾—è¯¦ç»†é”™è¯¯ä¿¡æ¯
python start_server.py

# æ£€æŸ¥æ‰€æœ‰ä¾èµ–æ˜¯å¦å®‰è£…
pip install -r requirements.txt

# éªŒè¯ç¯å¢ƒå˜é‡
python -c "from config.settings import get_settings; print(get_settings().database.url)"
```

### ç³»ç»Ÿæ¶æ„éªŒè¯

å®‰è£…æˆåŠŸåï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š

1. **Docker å®¹å™¨è¿è¡Œ**:
   ```bash
   $ sudo docker-compose ps
   NAME                      STATUS
   quant_trigle-postgres-1   Up (healthy)
   quant_trigle-redis-1      Up (healthy)
   ```

2. **æ•°æ®åº“è¡¨åˆ›å»º**:
   ```bash
   $ sudo docker-compose exec postgres psql -U postgres -d stock_analysis -c "\dt"
   # åº”è¯¥æ˜¾ç¤º 12 ä¸ªè¡¨
   ```

3. **API ç«¯ç‚¹å“åº”**:
   ```bash
   python start_server.py &
   $ curl http://localhost:8000/health
   {"status":"ok","database":"healthy","version":"0.1.0","environment":"development"}
   ```

### å¿«é€ŸéªŒè¯ç³»ç»ŸåŠŸèƒ½

å®‰è£…å®Œæˆåï¼Œå¯ä»¥å¿«é€ŸéªŒè¯æ ¸å¿ƒåŠŸèƒ½ï¼š

```bash
# 1. æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
curl http://localhost:8000/health

# 2. æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯
curl http://localhost:8000/api/v1/info

# 3. è®¿é—® API æ–‡æ¡£
# æµè§ˆå™¨æ‰“å¼€: http://localhost:8000/docs

# 4. è¿è¡Œå®Œæ•´æµ‹è¯•
python test_api.py
```

### ä¸‹ä¸€æ­¥

- ğŸ” æŸ¥çœ‹ [API æ–‡æ¡£](http://localhost:8000/docs) (Swagger UI)
- ğŸ“– é˜…è¯» [ä½¿ç”¨ç¤ºä¾‹](#-usage-examples)
- ğŸ› ï¸ æ¢ç´¢ [å¼€å‘æŒ‡å—](#-development)
- ğŸ“‹ æŸ¥çœ‹ [Docker è®¾ç½®è¯¦ç»†è®°å½•](DOCKER_SETUP_SUMMARY.md)

#### æ–¹å¼äºŒï¼šSQLite å®‰è£… (æµ‹è¯•/å¼€å‘)

é€‚ç”¨äºå¿«é€Ÿæµ‹è¯•æˆ–ä¸æƒ³ä½¿ç”¨ Docker çš„æƒ…å†µã€‚

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/stock-analysis-system.git
cd stock-analysis-system

# è®¾ç½® Python ç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# ä½¿ç”¨ SQLite æ•°æ®åº“æµ‹è¯•
python test_migration.py

# æˆ–ç›´æ¥è¿è¡Œè¿ç§»
DATABASE_URL="sqlite:///./stock_analysis.db" alembic upgrade head

# å¯åŠ¨åº”ç”¨ (åŠŸèƒ½å—é™ï¼Œæ—  Redis ç¼“å­˜)
DATABASE_URL="sqlite:///./stock_analysis.db" uvicorn stock_analysis_system.api.main:app --reload
```

#### æ–¹å¼ä¸‰ï¼šæœ¬åœ° PostgreSQL å®‰è£…

é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒæˆ–éœ€è¦å®Œå…¨æ§åˆ¶æ•°æ®åº“çš„æƒ…å†µã€‚

```bash
# å®‰è£… PostgreSQL (Ubuntu/Debian)
sudo apt update && sudo apt install postgresql postgresql-contrib redis-server

# åˆ›å»ºæ•°æ®åº“
sudo -u postgres psql -c "CREATE DATABASE stock_analysis;"
sudo -u postgres psql -c "CREATE USER postgres WITH PASSWORD 'password';"
sudo -u postgres psql -c "GRANT ALL PRIVILEGES ON DATABASE stock_analysis TO postgres;"

# å¯åŠ¨æœåŠ¡
sudo systemctl start postgresql redis-server
sudo systemctl enable postgresql redis-server

# è®¾ç½®åº”ç”¨
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶é…ç½®æ•°æ®åº“è¿æ¥
pip install -r requirements.txt
alembic upgrade head

# å¯åŠ¨åº”ç”¨
uvicorn stock_analysis_system.api.main:app --reload
```

> **ğŸ’¡ æç¤º**: æ¨èä½¿ç”¨ Docker æ–¹å¼ï¼Œå®ƒèƒ½è‡ªåŠ¨å¤„ç†æ‰€æœ‰ä¾èµ–å’Œé…ç½®ï¼Œé¿å…ç¯å¢ƒé—®é¢˜ã€‚

## ğŸ“Š Usage Examples

### Spring Festival Analysis (âœ… Available Now)

```python
from stock_analysis_system.analysis.spring_festival_engine import SpringFestivalAlignmentEngine
from stock_analysis_system.data.data_source_manager import DataSourceManager
from datetime import date

# Initialize components
engine = SpringFestivalAlignmentEngine(window_days=60)
data_manager = DataSourceManager()

# Get stock data
stock_data = await data_manager.get_stock_data("000001.SZ", 
                                               date(2020, 1, 1), 
                                               date(2024, 12, 31))

# Perform Spring Festival alignment analysis
aligned_data = engine.align_to_spring_festival(stock_data, years=[2020, 2021, 2022, 2023, 2024])
seasonal_pattern = engine.identify_seasonal_patterns(aligned_data)

print(f"Pattern strength: {seasonal_pattern.pattern_strength:.2f}")
print(f"Average return before SF: {seasonal_pattern.average_return_before:.2f}%")
print(f"Average return after SF: {seasonal_pattern.average_return_after:.2f}%")
print(f"Confidence level: {seasonal_pattern.confidence_level:.2f}")

# Generate trading signals
current_position = engine.get_current_position("000001.SZ")
signals = engine.generate_trading_signals(seasonal_pattern, current_position)
print(f"Signal: {signals['signal']} (strength: {signals['strength']:.2f})")
```

### Data Quality Analysis (âœ… Available Now)

```python
from stock_analysis_system.data.data_quality_engine import EnhancedDataQualityEngine

# Initialize quality engine
quality_engine = EnhancedDataQualityEngine()

# Train ML anomaly detector (optional)
quality_engine.train_ml_detector(stock_data, 
                                 feature_columns=['open_price', 'high_price', 'low_price', 'close_price', 'volume'])

# Validate data quality
quality_report = quality_engine.validate_data(stock_data, dataset_name="000001.SZ Daily Data")

print(f"Overall quality score: {quality_report.overall_score:.2f}")
print(f"Issues found: {len(quality_report.issues)}")
print(f"Recommendations: {quality_report.recommendations}")

# Clean data automatically
cleaned_data = quality_engine.clean_data(stock_data, quality_report)
```

### Multi-Source Data Access (âœ… Available Now)

```python
from stock_analysis_system.data.data_source_manager import DataSourceManager

# Initialize with automatic failover
data_manager = DataSourceManager()

# Get data with automatic source selection
stock_data = await data_manager.get_stock_data("000001.SZ", 
                                               date(2024, 1, 1), 
                                               date(2024, 12, 31))

# Get intraday data (if available)
intraday_data = await data_manager.get_intraday_data("000001.SZ", 
                                                     date(2024, 12, 1), 
                                                     date(2024, 12, 31), 
                                                     timeframe='5min')

# Check source health
health_status = data_manager.get_health_status()
for source, health in health_status.items():
    print(f"{source}: {health.status.value} (reliability: {health.reliability_score:.2f})")
```

## ğŸ§ª Testing

### Database Migration Testing

Test the database setup without requiring PostgreSQL:

```bash
# Test migration with SQLite (no database server required)
python test_migration.py

# Test with specific database URL
DATABASE_URL="sqlite:///./test.db" alembic upgrade head
DATABASE_URL="sqlite:///./test.db" alembic current
```

### Application Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=stock_analysis_system --cov-report=html

# Run only unit tests
pytest -m unit

# Run only integration tests
pytest -m integration

# Test with SQLite database
DATABASE_URL="sqlite:///./test.db" pytest
```

## ğŸ”§ Development

### Code Quality

This project uses several tools to maintain code quality:

- **Black**: Code formatting
- **isort**: Import sorting
- **flake8**: Linting
- **mypy**: Type checking
- **pre-commit**: Git hooks for quality checks

Run quality checks manually:

```bash
# Format code
black stock_analysis_system tests

# Sort imports
isort stock_analysis_system tests

# Lint code
flake8 stock_analysis_system tests

# Type checking
mypy stock_analysis_system

# Run all quality checks at once
make lint
```

### Development Setup

Use the automated setup script:

```bash
# Automated development environment setup
python scripts/setup_dev.py

# Or use make commands
make setup-dev    # Full development setup
make install-dev  # Install development dependencies
make test         # Run tests
make run-dev      # Start development server
```

### Project Structure

```
stock_analysis_system/
â”œâ”€â”€ analysis/           # Analysis engines (Spring Festival, Risk, etc.)
â”œâ”€â”€ api/               # FastAPI application and routes
â”œâ”€â”€ core/              # Core utilities and base classes
â”œâ”€â”€ data/              # Data access layer and ETL
â”œâ”€â”€ utils/             # Utility functions and helpers
â””â”€â”€ visualization/     # Chart generation and visualization

tests/
â”œâ”€â”€ unit/              # Unit tests
â”œâ”€â”€ integration/       # Integration tests
â””â”€â”€ fixtures/          # Test data and fixtures

config/                # Configuration files
docs/                  # Documentation
scripts/               # Utility scripts
```

## ğŸ“ˆ Performance

The system delivers high performance through:

- **Async Architecture**: Full async/await implementation with FastAPI and asyncio
- **Intelligent Caching**: Redis-powered caching with configurable TTL and cache warming
- **Circuit Breaker Pattern**: Automatic failover prevents cascade failures
- **Connection Pooling**: PostgreSQL connection pooling for optimal database performance
- **Batch Processing**: Celery-powered background tasks for large dataset processing
- **Rate Limiting**: Smart rate limiting prevents API throttling
- **Response Times**: 
  - API health checks: <100ms
  - Stock data queries: <500ms (cached), <2s (fresh)
  - Spring Festival analysis: <5s for 5-year dataset
  - Data quality validation: <3s for 1000 records

## ğŸ”’ Security

Security features include:

- **JWT Authentication**: Secure API access
- **Input Validation**: Pydantic models for data validation
- **SQL Injection Protection**: SQLAlchemy ORM
- **Rate Limiting**: API rate limiting
- **Encryption**: Sensitive data encryption

## ğŸ“š Documentation

- [API Documentation](docs/api.md)
- [Architecture Guide](docs/architecture.md)
- [Deployment Guide](docs/deployment.md)
- [Contributing Guidelines](CONTRIBUTING.md)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests and quality checks
5. Commit your changes (`git commit -m 'Add amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Chinese calendar calculations based on astronomical data
- Financial data provided by AkShare and Tushare
- Visualization powered by Plotly and D3.js
- Machine learning capabilities via scikit-learn and MLflow

## ğŸ“ Support

For support and questions:

- ğŸ“§ Email: support@stockanalysis.com
- ğŸ’¬ Discord: [Join our community](https://discord.gg/stockanalysis)
- ğŸ“– Documentation: [Read the docs](https://stock-analysis-system.readthedocs.io/)
- ğŸ› Issues: [GitHub Issues](https://github.com/your-org/stock-analysis-system/issues)
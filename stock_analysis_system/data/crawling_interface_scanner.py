"""
çˆ¬è™«æ¥å£æ‰«æå™¨

è¯¥æ¨¡å—è´Ÿè´£è‡ªåŠ¨æ‰«æ tmp/core/crawling/ ç›®å½•ä¸­çš„æ‰€æœ‰çˆ¬è™«æ¥å£ï¼Œ
æå–æ¥å£å…ƒæ•°æ®ï¼Œå¹¶ç”Ÿæˆæ¥å£å…¼å®¹æ€§æŠ¥å‘Šã€‚

Author: Stock Analysis System Team
Date: 2024-01-20
"""

import ast
import inspect
import importlib.util
import os
import sys
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import pandas as pd
import logging

logger = logging.getLogger(__name__)


@dataclass
class FunctionSignature:
    """å‡½æ•°ç­¾åä¿¡æ¯"""
    name: str
    parameters: List[Dict[str, Any]]
    return_type: str
    docstring: Optional[str]
    source_file: str
    line_number: int


@dataclass
class InterfaceMetadata:
    """æ¥å£å…ƒæ•°æ®"""
    module_name: str
    file_path: str
    functions: List[FunctionSignature]
    data_types: List[str]
    update_frequency: str
    reliability_score: float
    last_modified: datetime
    dependencies: List[str]
    api_endpoints: List[str]
    data_sources: List[str]


class CrawlingInterfaceScanner:
    """çˆ¬è™«æ¥å£æ‰«æå™¨"""
    
    def __init__(self, crawling_dir: str = "tmp/core/crawling"):
        """
        åˆå§‹åŒ–æ‰«æå™¨
        
        Args:
            crawling_dir: çˆ¬è™«æ¥å£ç›®å½•è·¯å¾„
        """
        self.crawling_dir = crawling_dir
        self.interfaces_metadata = {}
        self.compatibility_issues = []
        
        # æ•°æ®ç±»å‹æ˜ å°„
        self.data_type_keywords = {
            'stock': ['è‚¡ç¥¨', 'stock', 'equity', 'ä¸ªè‚¡', 'zh_a'],
            'fund': ['åŸºé‡‘', 'fund', 'etf', 'èµ„é‡‘'],
            'dragon_tiger': ['é¾™è™æ¦œ', 'lhb', 'billboard', 'dragon', 'tiger'],
            'limitup': ['æ¶¨åœ', 'limitup', 'limit_up', 'harden'],
            'chip': ['ç­¹ç ', 'chip', 'race'],
            'realtime': ['å®æ—¶', 'spot', 'realtime', 'real_time'],
            'history': ['å†å²', 'hist', 'history', 'historical'],
            'financial': ['è´¢åŠ¡', 'financial', 'finance']
        }
        
        # æ›´æ–°é¢‘ç‡å…³é”®è¯
        self.frequency_keywords = {
            'realtime': ['å®æ—¶', 'spot', 'realtime', 'current'],
            'daily': ['æ—¥', 'daily', 'day'],
            'weekly': ['å‘¨', 'weekly', 'week'],
            'monthly': ['æœˆ', 'monthly', 'month'],
            'quarterly': ['å­£', 'quarterly', 'quarter'],
            'yearly': ['å¹´', 'yearly', 'annual']
        }
    
    def scan_crawling_directory(self) -> Dict[str, InterfaceMetadata]:
        """
        æ‰«æçˆ¬è™«ç›®å½•ä¸­çš„æ‰€æœ‰æ¥å£
        
        Returns:
            æ¥å£å…ƒæ•°æ®å­—å…¸
        """
        logger.info(f"å¼€å§‹æ‰«æçˆ¬è™«æ¥å£ç›®å½•: {self.crawling_dir}")
        
        if not os.path.exists(self.crawling_dir):
            logger.error(f"çˆ¬è™«ç›®å½•ä¸å­˜åœ¨: {self.crawling_dir}")
            return {}
        
        python_files = [f for f in os.listdir(self.crawling_dir) 
                       if f.endswith('.py') and f != '__init__.py']
        
        logger.info(f"å‘ç° {len(python_files)} ä¸ªPythonæ–‡ä»¶")
        
        for file_name in python_files:
            try:
                file_path = os.path.join(self.crawling_dir, file_name)
                module_name = file_name[:-3]  # ç§»é™¤.pyæ‰©å±•å
                
                metadata = self._analyze_interface_file(file_path, module_name)
                if metadata:
                    self.interfaces_metadata[module_name] = metadata
                    logger.info(f"æˆåŠŸåˆ†ææ¥å£: {module_name}")
                
            except Exception as e:
                logger.error(f"åˆ†ææ¥å£æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")
                self.compatibility_issues.append({
                    'file': file_name,
                    'error': str(e),
                    'type': 'analysis_error'
                })
        
        logger.info(f"æ‰«æå®Œæˆï¼Œå…±åˆ†æ {len(self.interfaces_metadata)} ä¸ªæ¥å£")
        return self.interfaces_metadata
    
    def _analyze_interface_file(self, file_path: str, module_name: str) -> Optional[InterfaceMetadata]:
        """
        åˆ†æå•ä¸ªæ¥å£æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            module_name: æ¨¡å—åç§°
            
        Returns:
            æ¥å£å…ƒæ•°æ®
        """
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                source_code = f.read()
            
            # è§£æAST
            tree = ast.parse(source_code)
            
            # æå–å‡½æ•°ä¿¡æ¯
            functions = self._extract_functions(tree, source_code, file_path)
            
            # åˆ†ææ•°æ®ç±»å‹
            data_types = self._analyze_data_types(source_code, functions)
            
            # åˆ†ææ›´æ–°é¢‘ç‡
            update_frequency = self._analyze_update_frequency(source_code, functions)
            
            # è®¡ç®—å¯é æ€§è¯„åˆ†
            reliability_score = self._calculate_reliability_score(source_code, functions)
            
            # æå–ä¾èµ–
            dependencies = self._extract_dependencies(tree)
            
            # æå–APIç«¯ç‚¹
            api_endpoints = self._extract_api_endpoints(source_code)
            
            # åˆ†ææ•°æ®æº
            data_sources = self._analyze_data_sources(source_code, api_endpoints)
            
            # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            last_modified = datetime.fromtimestamp(os.path.getmtime(file_path))
            
            return InterfaceMetadata(
                module_name=module_name,
                file_path=file_path,
                functions=functions,
                data_types=data_types,
                update_frequency=update_frequency,
                reliability_score=reliability_score,
                last_modified=last_modified,
                dependencies=dependencies,
                api_endpoints=api_endpoints,
                data_sources=data_sources
            )
            
        except Exception as e:
            logger.error(f"åˆ†ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return None
    
    def _extract_functions(self, tree: ast.AST, source_code: str, file_path: str) -> List[FunctionSignature]:
        """æå–å‡½æ•°ç­¾åä¿¡æ¯"""
        functions = []
        source_lines = source_code.split('\n')
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # æå–å‚æ•°ä¿¡æ¯
                parameters = []
                for arg in node.args.args:
                    param_info = {
                        'name': arg.arg,
                        'type': self._get_annotation_string(arg.annotation) if arg.annotation else 'Any',
                        'default': None
                    }
                    parameters.append(param_info)
                
                # å¤„ç†é»˜è®¤å‚æ•°
                defaults = node.args.defaults
                if defaults:
                    for i, default in enumerate(defaults):
                        param_idx = len(parameters) - len(defaults) + i
                        if param_idx >= 0:
                            parameters[param_idx]['default'] = ast.unparse(default)
                
                # æå–è¿”å›ç±»å‹
                return_type = 'Any'
                if node.returns:
                    return_type = self._get_annotation_string(node.returns)
                
                # æå–æ–‡æ¡£å­—ç¬¦ä¸²
                docstring = ast.get_docstring(node)
                
                functions.append(FunctionSignature(
                    name=node.name,
                    parameters=parameters,
                    return_type=return_type,
                    docstring=docstring,
                    source_file=file_path,
                    line_number=node.lineno
                ))
        
        return functions
    
    def _get_annotation_string(self, annotation) -> str:
        """è·å–ç±»å‹æ³¨è§£å­—ç¬¦ä¸²"""
        try:
            return ast.unparse(annotation)
        except:
            return 'Any'
    
    def _analyze_data_types(self, source_code: str, functions: List[FunctionSignature]) -> List[str]:
        """åˆ†ææ•°æ®ç±»å‹"""
        data_types = set()
        
        # åŸºäºæ–‡ä»¶å†…å®¹åˆ†æ
        content_lower = source_code.lower()
        for data_type, keywords in self.data_type_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                data_types.add(data_type)
        
        # åŸºäºå‡½æ•°ååˆ†æ
        for func in functions:
            func_name_lower = func.name.lower()
            for data_type, keywords in self.data_type_keywords.items():
                if any(keyword in func_name_lower for keyword in keywords):
                    data_types.add(data_type)
        
        return list(data_types)
    
    def _analyze_update_frequency(self, source_code: str, functions: List[FunctionSignature]) -> str:
        """åˆ†ææ›´æ–°é¢‘ç‡"""
        content_lower = source_code.lower()
        
        # æ£€æŸ¥é¢‘ç‡å…³é”®è¯
        for frequency, keywords in self.frequency_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                return frequency
        
        # åŸºäºå‡½æ•°åæ¨æ–­
        for func in functions:
            func_name_lower = func.name.lower()
            for frequency, keywords in self.frequency_keywords.items():
                if any(keyword in func_name_lower for keyword in keywords):
                    return frequency
        
        return 'unknown'
    
    def _calculate_reliability_score(self, source_code: str, functions: List[FunctionSignature]) -> float:
        """è®¡ç®—å¯é æ€§è¯„åˆ†"""
        score = 50.0  # åŸºç¡€åˆ†æ•°
        
        # æœ‰æ–‡æ¡£å­—ç¬¦ä¸²åŠ åˆ†
        documented_functions = sum(1 for func in functions if func.docstring)
        if functions:
            doc_ratio = documented_functions / len(functions)
            score += doc_ratio * 20
        
        # æœ‰ç±»å‹æ³¨è§£åŠ åˆ†
        typed_functions = sum(1 for func in functions 
                            if any(param['type'] != 'Any' for param in func.parameters))
        if functions:
            type_ratio = typed_functions / len(functions)
            score += type_ratio * 15
        
        # æœ‰é”™è¯¯å¤„ç†åŠ åˆ†
        if 'try:' in source_code and 'except' in source_code:
            score += 10
        
        # æœ‰æ—¥å¿—è®°å½•åŠ åˆ†
        if 'logging' in source_code or 'logger' in source_code:
            score += 5
        
        return min(score, 100.0)
    
    def _extract_dependencies(self, tree: ast.AST) -> List[str]:
        """æå–ä¾èµ–åŒ…"""
        dependencies = set()
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    dependencies.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    dependencies.add(node.module.split('.')[0])
        
        return list(dependencies)
    
    def _extract_api_endpoints(self, source_code: str) -> List[str]:
        """æå–APIç«¯ç‚¹"""
        endpoints = []
        
        # æŸ¥æ‰¾URLæ¨¡å¼
        import re
        url_pattern = r'["\']https?://[^"\']+["\']'
        urls = re.findall(url_pattern, source_code)
        
        for url in urls:
            clean_url = url.strip('"\'')
            endpoints.append(clean_url)
        
        return endpoints
    
    def _analyze_data_sources(self, source_code: str, api_endpoints: List[str]) -> List[str]:
        """åˆ†ææ•°æ®æº"""
        sources = set()
        
        # åŸºäºAPIç«¯ç‚¹åˆ†æ
        for endpoint in api_endpoints:
            if 'eastmoney' in endpoint:
                sources.add('ä¸œæ–¹è´¢å¯Œ')
            elif 'sina' in endpoint:
                sources.add('æ–°æµªè´¢ç»')
            elif '10jqka' in endpoint:
                sources.add('åŒèŠ±é¡º')
            elif 'tencent' in endpoint:
                sources.add('è…¾è®¯è´¢ç»')
        
        # åŸºäºä»£ç å†…å®¹åˆ†æ
        content_lower = source_code.lower()
        if 'eastmoney' in content_lower:
            sources.add('ä¸œæ–¹è´¢å¯Œ')
        if 'sina' in content_lower:
            sources.add('æ–°æµªè´¢ç»')
        if '10jqka' in content_lower or 'tonghuashun' in content_lower:
            sources.add('åŒèŠ±é¡º')
        
        return list(sources)
    
    def check_interface_compatibility(self, interface_name: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ¥å£å…¼å®¹æ€§"""
        if interface_name not in self.interfaces_metadata:
            return {'compatible': False, 'error': 'Interface not found'}
        
        metadata = self.interfaces_metadata[interface_name]
        compatibility_report = {
            'compatible': True,
            'issues': [],
            'recommendations': []
        }
        
        # æ£€æŸ¥å¿…è¦çš„ä¾èµ–
        required_deps = ['pandas', 'requests']
        missing_deps = [dep for dep in required_deps if dep not in metadata.dependencies]
        if missing_deps:
            compatibility_report['issues'].append(f"ç¼ºå°‘å¿…è¦ä¾èµ–: {missing_deps}")
        
        # æ£€æŸ¥å‡½æ•°è¿”å›ç±»å‹
        for func in metadata.functions:
            if func.return_type == 'Any':
                compatibility_report['recommendations'].append(
                    f"å»ºè®®ä¸ºå‡½æ•° {func.name} æ·»åŠ è¿”å›ç±»å‹æ³¨è§£"
                )
        
        # æ£€æŸ¥å¯é æ€§è¯„åˆ†
        if metadata.reliability_score < 60:
            compatibility_report['issues'].append(
                f"å¯é æ€§è¯„åˆ†è¾ƒä½: {metadata.reliability_score:.1f}"
            )
            compatibility_report['recommendations'].append("å»ºè®®æ·»åŠ æ›´å¤šæ–‡æ¡£å’Œé”™è¯¯å¤„ç†")
        
        return compatibility_report
    
    def generate_interface_report(self) -> pd.DataFrame:
        """ç”Ÿæˆæ¥å£æŠ¥å‘Š"""
        if not self.interfaces_metadata:
            return pd.DataFrame()
        
        report_data = []
        for name, metadata in self.interfaces_metadata.items():
            report_data.append({
                'æ¥å£åç§°': name,
                'å‡½æ•°æ•°é‡': len(metadata.functions),
                'æ•°æ®ç±»å‹': ', '.join(metadata.data_types),
                'æ›´æ–°é¢‘ç‡': metadata.update_frequency,
                'å¯é æ€§è¯„åˆ†': metadata.reliability_score,
                'æ•°æ®æº': ', '.join(metadata.data_sources),
                'ä¾èµ–åŒ…æ•°é‡': len(metadata.dependencies),
                'APIç«¯ç‚¹æ•°é‡': len(metadata.api_endpoints),
                'æœ€åä¿®æ”¹æ—¶é—´': metadata.last_modified.strftime('%Y-%m-%d %H:%M:%S')
            })
        
        return pd.DataFrame(report_data)
    
    def get_interface_metadata_dict(self) -> Dict[str, Dict]:
        """è·å–æ¥å£å…ƒæ•°æ®å­—å…¸æ ¼å¼"""
        return {name: asdict(metadata) for name, metadata in self.interfaces_metadata.items()}
    
    def export_metadata_to_json(self, output_path: str):
        """å¯¼å‡ºå…ƒæ•°æ®åˆ°JSONæ–‡ä»¶"""
        import json
        
        # è½¬æ¢datetimeå¯¹è±¡ä¸ºå­—ç¬¦ä¸²
        def datetime_converter(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
        
        metadata_dict = self.get_interface_metadata_dict()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metadata_dict, f, ensure_ascii=False, indent=2, default=datetime_converter)
        
        logger.info(f"æ¥å£å…ƒæ•°æ®å·²å¯¼å‡ºåˆ°: {output_path}")


# æµ‹è¯•å’Œä½¿ç”¨ç¤ºä¾‹
def test_crawling_interface_scanner():
    """æµ‹è¯•çˆ¬è™«æ¥å£æ‰«æå™¨"""
    print("ğŸ” æµ‹è¯•çˆ¬è™«æ¥å£æ‰«æå™¨")
    print("=" * 50)
    
    scanner = CrawlingInterfaceScanner()
    
    # æ‰«ææ¥å£
    print("1. æ‰«æçˆ¬è™«æ¥å£...")
    interfaces = scanner.scan_crawling_directory()
    print(f"   å‘ç° {len(interfaces)} ä¸ªæ¥å£")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("2. ç”Ÿæˆæ¥å£æŠ¥å‘Š...")
    report = scanner.generate_interface_report()
    print(f"   æŠ¥å‘ŠåŒ…å« {len(report)} è¡Œæ•°æ®")
    print("\næ¥å£æ¦‚è§ˆ:")
    print(report.to_string(index=False))
    
    # æ£€æŸ¥å…¼å®¹æ€§
    print("\n3. æ£€æŸ¥æ¥å£å…¼å®¹æ€§...")
    for interface_name in list(interfaces.keys())[:3]:  # æ£€æŸ¥å‰3ä¸ªæ¥å£
        compatibility = scanner.check_interface_compatibility(interface_name)
        print(f"   {interface_name}: {'âœ…' if compatibility['compatible'] else 'âŒ'}")
        if compatibility.get('issues'):
            for issue in compatibility['issues']:
                print(f"     é—®é¢˜: {issue}")
    
    # å¯¼å‡ºå…ƒæ•°æ®
    print("\n4. å¯¼å‡ºå…ƒæ•°æ®...")
    output_path = "crawling_interfaces_metadata.json"
    scanner.export_metadata_to_json(output_path)
    
    print("\nâœ… çˆ¬è™«æ¥å£æ‰«æå™¨æµ‹è¯•å®Œæˆ!")
    return scanner


if __name__ == "__main__":
    test_crawling_interface_scanner()
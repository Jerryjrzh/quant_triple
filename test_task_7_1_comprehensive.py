#!/usr/bin/env python3
"""
Comprehensive Test Suite for Task 7.1: MLflow Integration for Model Lifecycle

This test suite provides comprehensive testing for the ML Model Manager implementation,
focusing on MLflow integration and model lifecycle management capabilities.
"""

import asyncio
import json
import shutil
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
from unittest.mock import Mock, patch

import mlflow
import numpy as np
import pandas as pd
import pytest
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from stock_analysis_system.analysis.ml_model_manager import (
    DriftDetectionResult,
    MLModelManager,
    ModelInfo,
    ModelMetrics,
)


class TestTask71MLflowIntegration:
    """Comprehensive test suite for Task 7.1 MLflow integration."""

    @pytest.fixture
    def temp_mlflow_dir(self):
        """Create temporary directory for MLflow tracking."""
        temp_dir = tempfile.mkdtemp()
        mlflow_uri = f"file://{temp_dir}/mlruns"
        yield mlflow_uri
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def sample_data(self):
        """Create sample data for testing."""
        X, y = make_classification(
            n_samples=1000,
            n_features=10,
            n_informative=5,
            n_redundant=2,
            n_classes=2,
            random_state=42,
        )

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        return {
            "X_train": X_train,
            "X_test": X_test,
            "y_train": y_train,
            "y_test": y_test,
            "feature_names": [f"feature_{i}" for i in range(10)],
        }

    @pytest.fixture
    def sample_models(self, sample_data):
        """Create sample trained models."""
        # Random Forest model
        rf_model = RandomForestClassifier(n_estimators=10, random_state=42)
        rf_model.fit(sample_data["X_train"], sample_data["y_train"])

        # Logistic Regression model
        lr_model = LogisticRegression(random_state=42)
        lr_model.fit(sample_data["X_train"], sample_data["y_train"])

        return {"random_forest": rf_model, "logistic_regression": lr_model}

    @pytest.fixture
    def sample_metrics(self):
        """Create sample model metrics."""
        return {
            "high_performance": ModelMetrics(
                accuracy=0.95,
                precision=0.93,
                recall=0.97,
                f1_score=0.95,
                custom_metrics={"auc_roc": 0.98, "log_loss": 0.15},
            ),
            "medium_performance": ModelMetrics(
                accuracy=0.85,
                precision=0.83,
                recall=0.87,
                f1_score=0.85,
                custom_metrics={"auc_roc": 0.89, "log_loss": 0.35},
            ),
            "low_performance": ModelMetrics(
                accuracy=0.65,
                precision=0.63,
                recall=0.67,
                f1_score=0.65,
                custom_metrics={"auc_roc": 0.69, "log_loss": 0.55},
            ),
        }

    @pytest.fixture
    def ml_manager(self, temp_mlflow_dir):
        """Create ML Model Manager instance for testing."""
        return MLModelManager(mlflow_tracking_uri=temp_mlflow_dir)

    def test_mlflow_initialization(self, temp_mlflow_dir):
        """Test MLflow tracking server initialization."""
        manager = MLModelManager(mlflow_tracking_uri=temp_mlflow_dir)

        assert manager.mlflow_uri == temp_mlflow_dir
        assert manager.drift_threshold > 0
        assert isinstance(manager.models, dict)
        assert isinstance(manager.retraining_schedule, dict)
        assert manager.client is not None

        # Verify MLflow experiment is created
        experiment = mlflow.get_experiment_by_name(manager.settings.ml.experiment_name)
        assert experiment is not None

    @pytest.mark.asyncio
    async def test_model_registration_basic(
        self, ml_manager, sample_models, sample_metrics
    ):
        """Test basic model registration functionality."""
        model_name = "test_basic_registration"
        model = sample_models["random_forest"]
        metrics = sample_metrics["high_performance"]

        model_id = await ml_manager.register_model(
            model_name=model_name,
            model_object=model,
            metrics=metrics,
        )

        # Verify model registration
        assert model_id is not None
        assert model_id.startswith(f"{model_name}_v")
        assert model_id in ml_manager.models

        # Verify model info
        model_info = ml_manager.models[model_id]
        assert model_info.model_name == model_name
        assert model_info.status == "staging"
        assert model_info.metrics.accuracy == metrics.accuracy

    @pytest.mark.asyncio
    async def test_model_registration_with_metadata(
        self, ml_manager, sample_models, sample_metrics
    ):
        """Test model registration with comprehensive metadata."""
        model_name = "test_metadata_registration"
        model = sample_models["logistic_regression"]
        metrics = sample_metrics["medium_performance"]
        tags = {
            "version": "1.0",
            "type": "classification",
            "algorithm": "logistic_regression",
            "dataset": "synthetic",
        }
        description = "Test model with comprehensive metadata"

        # Create sample artifacts
        artifacts = {
            "feature_importance": {
                "feature_0": 0.15,
                "feature_1": 0.12,
                "feature_2": 0.18,
            },
            "training_config": {
                "max_iter": 1000,
                "solver": "lbfgs",
                "random_state": 42,
            },
            "validation_results": pd.DataFrame(
                {
                    "fold": [1, 2, 3, 4, 5],
                    "accuracy": [0.85, 0.87, 0.83, 0.86, 0.84],
                    "f1_score": [0.84, 0.86, 0.82, 0.85, 0.83],
                }
            ),
        }

        model_id = await ml_manager.register_model(
            model_name=model_name,
            model_object=model,
            metrics=metrics,
            tags=tags,
            description=description,
            artifacts=artifacts,
        )

        # Verify registration with metadata
        assert model_id is not None
        model_info = ml_manager.models[model_id]
        assert model_info.tags == tags
        assert model_info.description == description

        # Verify MLflow tracking
        try:
            experiment = mlflow.get_experiment_by_name(ml_manager.settings.ml.experiment_name)
            runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
            assert len(runs) > 0
            print(f"   ‚úì Found {len(runs)} MLflow runs")
        except Exception as e:
            print(f"   ‚ö† MLflow tracking verification skipped: {e}")

    @pytest.mark.asyncio
    async def test_model_promotion_workflow(
        self, ml_manager, sample_models, sample_metrics
    ):
        """Test complete model promotion workflow."""
        # Register a model
        model_id = await ml_manager.register_model(
            model_name="test_promotion_workflow",
            model_object=sample_models["random_forest"],
            metrics=sample_metrics["high_performance"],
        )

        # Verify initial status
        assert ml_manager.models[model_id].status == "staging"

        # Promote to production
        success = await ml_manager.promote_model_to_production(model_id)
        assert success is True
        assert ml_manager.models[model_id].status == "production"

        # Verify MLflow model registry status
        model_info = ml_manager.models[model_id]
        latest_versions = ml_manager.client.get_latest_versions(
            model_info.model_name, stages=["Production"]
        )
        assert len(latest_versions) > 0
        assert latest_versions[0].current_stage == "Production"

    @pytest.mark.asyncio
    async def test_model_promotion_with_existing_production(
        self, ml_manager, sample_models, sample_metrics
    ):
        """Test model promotion when production model already exists."""
        # Register and promote first model
        model_id_1 = await ml_manager.register_model(
            model_name="test_existing_prod_1",
            model_object=sample_models["random_forest"],
            metrics=sample_metrics["medium_performance"],
        )
        await ml_manager.promote_model_to_production(model_id_1)

        # Register and promote second model
        model_id_2 = await ml_manager.register_model(
            model_name="test_existing_prod_1",  # Same name
            model_object=sample_models["logistic_regression"],
            metrics=sample_metrics["high_performance"],
        )
        success = await ml_manager.promote_model_to_production(model_id_2)

        assert success is True
        assert ml_manager.models[model_id_2].status == "production"

        # Verify only one production model exists
        model_info = ml_manager.models[model_id_2]
        prod_versions = ml_manager.client.get_latest_versions(
            model_info.model_name, stages=["Production"]
        )
        assert len(prod_versions) == 1

    @pytest.mark.asyncio
    async def test_model_loading(self, ml_manager, sample_models, sample_metrics, sample_data):
        """Test model loading from MLflow."""
        # Register and promote a model
        model_id = await ml_manager.register_model(
            model_name="test_model_loading",
            model_object=sample_models["random_forest"],
            metrics=sample_metrics["high_performance"],
        )
        await ml_manager.promote_model_to_production(model_id)

        # Load the model
        loaded_model = await ml_manager.load_model(model_id, stage="Production")

        # Verify loaded model functionality
        assert loaded_model is not None
        assert hasattr(loaded_model, "predict")

        # Test predictions
        predictions = loaded_model.predict(sample_data["X_test"])
        assert len(predictions) == len(sample_data["X_test"])
        assert all(pred in [0, 1] for pred in predictions)

    @pytest.mark.asyncio
    async def test_model_versioning(self, ml_manager, sample_models, sample_metrics):
        """Test model versioning functionality."""
        model_name = "test_versioning"

        # Register multiple versions of the same model
        model_ids = []
        for i, (model_type, model) in enumerate(sample_models.items()):
            model_id = await ml_manager.register_model(
                model_name=model_name,
                model_object=model,
                metrics=sample_metrics["medium_performance"],
                tags={"version": f"1.{i}", "model_type": model_type},
            )
            model_ids.append(model_id)

        # Verify different versions exist
        assert len(model_ids) == 2
        assert all(mid.startswith(f"{model_name}_v") for mid in model_ids)

        # Verify versions are different
        versions = [ml_manager.models[mid].version for mid in model_ids]
        assert len(set(versions)) == 2

    @pytest.mark.asyncio
    async def test_model_comparison(
        self, ml_manager, sample_models, sample_metrics, sample_data
    ):
        """Test model comparison functionality."""
        # Register multiple models
        model_ids = []
        for i, (model_type, model) in enumerate(sample_models.items()):
            model_id = await ml_manager.register_model(
                model_name=f"test_comparison_{model_type}",
                model_object=model,
                metrics=sample_metrics["medium_performance"],
            )
            await ml_manager.promote_model_to_production(model_id)
            model_ids.append(model_id)

        # Compare models
        comparison_results = await ml_manager.compare_models(
            model_ids=model_ids,
            test_data=sample_data["X_test"],
            test_labels=sample_data["y_test"],
            metrics=["accuracy", "precision", "recall", "f1_score"],
        )

        # Verify comparison results
        assert len(comparison_results) == len(model_ids)
        for model_id in model_ids:
            assert model_id in comparison_results
            metrics = comparison_results[model_id]
            assert "accuracy" in metrics
            assert "precision" in metrics
            assert "recall" in metrics
            assert "f1_score" in metrics
            assert all(0 <= v <= 1 for v in metrics.values())

    @pytest.mark.asyncio
    async def test_drift_detection_no_drift(self, ml_manager, sample_data):
        """Test drift detection when no drift is present."""
        model_id = "test_no_drift"

        # Add model to manager
        ml_manager.models[model_id] = ModelInfo(
            model_id=model_id,
            model_name="drift_test",
            version="1",
            status="production",
            created_at=datetime.now(),
            last_updated=datetime.now(),
            metrics=ModelMetrics(0.8, 0.8, 0.8, 0.8, {}),
            drift_score=0.0,
            tags={},
        )

        # Use similar distributions
        reference_data = sample_data["X_train"][:400]
        new_data = sample_data["X_train"][400:600]

        result = await ml_manager.detect_model_drift(
            model_id=model_id,
            new_data=new_data,
            reference_data=reference_data,
            feature_names=sample_data["feature_names"],
        )

        assert isinstance(result, DriftDetectionResult)
        assert result.drift_type == "data"
        assert result.drift_score >= 0.0
        assert "overall_drift_score" in result.details
        assert "feature_drift_details" in result.details
        assert len(result.details["feature_drift_details"]) == len(sample_data["feature_names"])

    @pytest.mark.asyncio
    async def test_drift_detection_with_drift(self, ml_manager, sample_data):
        """Test drift detection when significant drift is present."""
        model_id = "test_with_drift"

        # Add model to manager
        ml_manager.models[model_id] = ModelInfo(
            model_id=model_id,
            model_name="drift_test_with_drift",
            version="1",
            status="production",
            created_at=datetime.now(),
            last_updated=datetime.now(),
            metrics=ModelMetrics(0.8, 0.8, 0.8, 0.8, {}),
            drift_score=0.0,
            tags={},
        )

        # Create reference data
        reference_data = sample_data["X_train"][:400]

        # Create significantly drifted data
        new_data = sample_data["X_train"][400:600].copy()
        new_data = new_data + np.random.normal(0, 3, new_data.shape)  # Add noise
        new_data = new_data + 10  # Shift distribution significantly

        result = await ml_manager.detect_model_drift(
            model_id=model_id,
            new_data=new_data,
            reference_data=reference_data,
            feature_names=sample_data["feature_names"],
        )

        assert isinstance(result, DriftDetectionResult)
        assert result.drift_score > 0.0
        # With significant drift, it should be detected
        assert result.drift_detected is True or result.drift_score > 0.2

    @pytest.mark.asyncio
    async def test_retraining_scheduling(self, ml_manager, sample_models, sample_metrics):
        """Test automated retraining scheduling."""
        # Register a model
        model_id = await ml_manager.register_model(
            model_name="test_retraining_schedule",
            model_object=sample_models["random_forest"],
            metrics=sample_metrics["medium_performance"],
        )

        # Test periodic scheduling
        await ml_manager.schedule_retraining(
            model_id=model_id,
            schedule_type="periodic",
            schedule_config={"interval_days": 7},
        )

        assert model_id in ml_manager.retraining_schedule
        schedule_info = ml_manager.retraining_schedule[model_id]
        assert schedule_info["schedule_type"] == "periodic"
        assert schedule_info["schedule_config"]["interval_days"] == 7

        # Test drift-based scheduling
        await ml_manager.schedule_retraining(
            model_id=model_id,
            schedule_type="drift_based",
        )

        schedule_info = ml_manager.retraining_schedule[model_id]
        assert schedule_info["schedule_type"] == "drift_based"

    @pytest.mark.asyncio
    async def test_retraining_due_check(self, ml_manager, sample_models, sample_metrics):
        """Test checking for models due for retraining."""
        # Register a model
        model_id = await ml_manager.register_model(
            model_name="test_retraining_due",
            model_object=sample_models["random_forest"],
            metrics=sample_metrics["medium_performance"],
        )

        # Set up past due retraining schedule
        past_date = datetime.now() - timedelta(days=1)
        ml_manager.retraining_schedule[model_id] = {
            "schedule_type": "periodic",
            "schedule_config": {"interval_days": 7},
            "last_retrain": past_date,
            "next_retrain": past_date,
            "retrain_count": 0,
        }

        due_models = await ml_manager.check_retraining_due()
        assert model_id in due_models

        # Test drift-based retraining due check
        ml_manager.retraining_schedule[model_id]["schedule_type"] = "drift_based"
        ml_manager.models[model_id].drift_score = 0.8  # High drift score

        due_models = await ml_manager.check_retraining_due()
        assert model_id in due_models

    @pytest.mark.asyncio
    async def test_model_archiving(self, ml_manager, sample_models, sample_metrics):
        """Test model archiving functionality."""
        # Register a model
        model_id = await ml_manager.register_model(
            model_name="test_archiving",
            model_object=sample_models["random_forest"],
            metrics=sample_metrics["medium_performance"],
        )

        # Archive the model
        success = await ml_manager.archive_model(model_id)

        assert success is True
        assert ml_manager.models[model_id].status == "archived"

        # Verify MLflow model registry status
        model_info = ml_manager.models[model_id]
        archived_versions = ml_manager.client.get_latest_versions(
            model_info.model_name, stages=["Archived"]
        )
        assert len(archived_versions) > 0

    @pytest.mark.asyncio
    async def test_model_listing_and_filtering(
        self, ml_manager, sample_models, sample_metrics
    ):
        """Test model listing with various filters."""
        # Register multiple models with different statuses
        model_ids = []
        for i, (model_type, model) in enumerate(sample_models.items()):
            model_id = await ml_manager.register_model(
                model_name=f"test_listing_{model_type}",
                model_object=model,
                metrics=sample_metrics["medium_performance"],
                tags={"type": model_type, "batch": "test_batch"},
            )
            model_ids.append(model_id)

        # Promote one to production
        await ml_manager.promote_model_to_production(model_ids[0])

        # Archive one
        await ml_manager.archive_model(model_ids[1])

        # Test listing all models
        all_models = await ml_manager.list_models()
        assert len(all_models) >= 2

        # Test filtering by status
        prod_models = await ml_manager.list_models(status_filter="production")
        staging_models = await ml_manager.list_models(status_filter="staging")
        archived_models = await ml_manager.list_models(status_filter="archived")

        assert len(prod_models) >= 1
        assert len(archived_models) >= 1

        # Test filtering by name
        filtered_models = await ml_manager.list_models(name_filter="test_listing")
        assert len(filtered_models) >= 2

    @pytest.mark.asyncio
    async def test_model_info_retrieval(self, ml_manager, sample_models, sample_metrics):
        """Test model information retrieval."""
        # Register a model with comprehensive info
        model_id = await ml_manager.register_model(
            model_name="test_info_retrieval",
            model_object=sample_models["random_forest"],
            metrics=sample_metrics["high_performance"],
            tags={"version": "1.0", "type": "classification"},
            description="Test model for info retrieval",
        )

        # Retrieve model info
        model_info = await ml_manager.get_model_info(model_id)

        assert model_info is not None
        assert model_info.model_id == model_id
        assert model_info.model_name == "test_info_retrieval"
        assert model_info.status == "staging"
        assert model_info.metrics.accuracy == sample_metrics["high_performance"].accuracy
        assert model_info.tags["version"] == "1.0"
        assert model_info.description == "Test model for info retrieval"

        # Test serialization
        info_dict = model_info.to_dict()
        assert isinstance(info_dict, dict)
        assert info_dict["model_id"] == model_id
        assert info_dict["accuracy"] in info_dict["metrics"]

    @pytest.mark.asyncio
    async def test_error_handling(self, ml_manager):
        """Test error handling in various scenarios."""
        # Test promoting non-existent model
        success = await ml_manager.promote_model_to_production("non_existent_model")
        assert success is False

        # Test archiving non-existent model
        success = await ml_manager.archive_model("non_existent_model")
        assert success is False

        # Test loading non-existent model
        model = await ml_manager.load_model("non_existent_model")
        assert model is None

        # Test getting info for non-existent model
        info = await ml_manager.get_model_info("non_existent_model")
        assert info is None

        # Test drift detection with invalid model
        result = await ml_manager.detect_model_drift(
            model_id="non_existent_model",
            new_data=np.random.rand(10, 5),
            reference_data=np.random.rand(10, 5),
        )
        # The drift detection should handle the error gracefully
        assert isinstance(result, DriftDetectionResult)
        print(f"   ‚úì Drift detection error handling: confidence={result.confidence}, drift_type={result.drift_type}")

    def test_model_metrics_functionality(self):
        """Test ModelMetrics class functionality."""
        metrics = ModelMetrics(
            accuracy=0.85,
            precision=0.83,
            recall=0.87,
            f1_score=0.85,
            custom_metrics={"auc_roc": 0.89, "log_loss": 0.35},
        )

        # Test basic properties
        assert metrics.accuracy == 0.85
        assert metrics.precision == 0.83
        assert metrics.recall == 0.87
        assert metrics.f1_score == 0.85

        # Test custom metrics
        assert metrics.custom_metrics["auc_roc"] == 0.89
        assert metrics.custom_metrics["log_loss"] == 0.35

        # Test serialization
        metrics_dict = metrics.to_dict()
        assert metrics_dict["accuracy"] == 0.85
        assert metrics_dict["auc_roc"] == 0.89
        assert metrics_dict["log_loss"] == 0.35

    def test_drift_detection_result_functionality(self):
        """Test DriftDetectionResult class functionality."""
        result = DriftDetectionResult(
            drift_detected=True,
            drift_score=0.25,
            drift_type="data",
            confidence=0.85,
            details={
                "feature_count": 10,
                "significant_features": ["feature_1", "feature_3"],
                "drift_threshold": 0.2,
            },
        )

        assert result.drift_detected is True
        assert result.drift_score == 0.25
        assert result.drift_type == "data"
        assert result.confidence == 0.85
        assert result.details["feature_count"] == 10
        assert "feature_1" in result.details["significant_features"]


async def run_comprehensive_test():
    """Run comprehensive test suite for Task 7.1."""
    print("üöÄ Starting Comprehensive Test Suite for Task 7.1: MLflow Integration")
    print("=" * 80)

    # Create test instance
    test_instance = TestTask71MLflowIntegration()

    # Create temporary MLflow directory
    temp_dir = tempfile.mkdtemp()
    mlflow_uri = f"file://{temp_dir}/mlruns"

    try:
        # Initialize manager
        manager = MLModelManager(mlflow_tracking_uri=mlflow_uri)

        # Create sample data
        X, y = make_classification(
            n_samples=1000, n_features=10, n_informative=5, n_classes=2, random_state=42
        )
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        sample_data = {
            "X_train": X_train,
            "X_test": X_test,
            "y_train": y_train,
            "y_test": y_test,
            "feature_names": [f"feature_{i}" for i in range(10)],
        }

        # Create sample models
        rf_model = RandomForestClassifier(n_estimators=10, random_state=42)
        rf_model.fit(X_train, y_train)

        lr_model = LogisticRegression(random_state=42)
        lr_model.fit(X_train, y_train)

        sample_models = {"random_forest": rf_model, "logistic_regression": lr_model}

        # Create sample metrics
        sample_metrics = {
            "high_performance": ModelMetrics(0.95, 0.93, 0.97, 0.95, {"auc_roc": 0.98}),
            "medium_performance": ModelMetrics(0.85, 0.83, 0.87, 0.85, {"auc_roc": 0.89}),
        }

        print("‚úÖ Test environment initialized successfully")

        # Test 1: MLflow Initialization
        print("\nüìã Test 1: MLflow Initialization")
        test_instance.test_mlflow_initialization(mlflow_uri)
        print("‚úÖ MLflow initialization test passed")

        # Test 2: Basic Model Registration
        print("\nüìã Test 2: Basic Model Registration")
        model_id = await test_instance.test_model_registration_basic(
            manager, sample_models, sample_metrics
        )
        print("‚úÖ Basic model registration test passed")

        # Test 3: Model Registration with Metadata
        print("\nüìã Test 3: Model Registration with Metadata")
        await test_instance.test_model_registration_with_metadata(
            manager, sample_models, sample_metrics
        )
        print("‚úÖ Model registration with metadata test passed")

        # Test 4: Model Promotion Workflow
        print("\nüìã Test 4: Model Promotion Workflow")
        await test_instance.test_model_promotion_workflow(
            manager, sample_models, sample_metrics
        )
        print("‚úÖ Model promotion workflow test passed")

        # Test 5: Model Loading
        print("\nüìã Test 5: Model Loading")
        await test_instance.test_model_loading(
            manager, sample_models, sample_metrics, sample_data
        )
        print("‚úÖ Model loading test passed")

        # Test 6: Model Versioning
        print("\nüìã Test 6: Model Versioning")
        await test_instance.test_model_versioning(
            manager, sample_models, sample_metrics
        )
        print("‚úÖ Model versioning test passed")

        # Test 7: Model Comparison
        print("\nüìã Test 7: Model Comparison")
        await test_instance.test_model_comparison(
            manager, sample_models, sample_metrics, sample_data
        )
        print("‚úÖ Model comparison test passed")

        # Test 8: Drift Detection
        print("\nüìã Test 8: Drift Detection")
        await test_instance.test_drift_detection_no_drift(manager, sample_data)
        await test_instance.test_drift_detection_with_drift(manager, sample_data)
        print("‚úÖ Drift detection tests passed")

        # Test 9: Retraining Scheduling
        print("\nüìã Test 9: Retraining Scheduling")
        await test_instance.test_retraining_scheduling(
            manager, sample_models, sample_metrics
        )
        await test_instance.test_retraining_due_check(
            manager, sample_models, sample_metrics
        )
        print("‚úÖ Retraining scheduling tests passed")

        # Test 10: Model Archiving
        print("\nüìã Test 10: Model Archiving")
        await test_instance.test_model_archiving(
            manager, sample_models, sample_metrics
        )
        print("‚úÖ Model archiving test passed")

        # Test 11: Model Listing and Filtering
        print("\nüìã Test 11: Model Listing and Filtering")
        await test_instance.test_model_listing_and_filtering(
            manager, sample_models, sample_metrics
        )
        print("‚úÖ Model listing and filtering test passed")

        # Test 12: Error Handling
        print("\nüìã Test 12: Error Handling")
        await test_instance.test_error_handling(manager)
        print("‚úÖ Error handling test passed")

        # Test 13: Data Classes
        print("\nüìã Test 13: Data Classes Functionality")
        test_instance.test_model_metrics_functionality()
        test_instance.test_drift_detection_result_functionality()
        print("‚úÖ Data classes functionality tests passed")

        print("\n" + "=" * 80)
        print("üéâ ALL TESTS PASSED! Task 7.1 MLflow Integration is working correctly!")
        print("=" * 80)

        # Print summary statistics
        print("\nüìä Test Summary:")
        print(f"   ‚Ä¢ Total models registered: {len(manager.models)}")
        print(f"   ‚Ä¢ MLflow tracking URI: {manager.mlflow_uri}")
        print(f"   ‚Ä¢ Drift threshold: {manager.drift_threshold}")
        print(f"   ‚Ä¢ Retraining schedules: {len(manager.retraining_schedule)}")

        return True

    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        # Cleanup
        shutil.rmtree(temp_dir)
        print("\nüßπ Cleanup completed")


if __name__ == "__main__":
    # Run the comprehensive test
    result = asyncio.run(run_comprehensive_test())
    exit(0 if result else 1)
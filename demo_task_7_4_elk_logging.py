#!/usr/bin/env python3
"""
Task 7.4 ELKæ—¥å¿—åˆ†æç³»ç»Ÿæ¼”ç¤º

æœ¬æ¼”ç¤ºè„šæœ¬å±•ç¤ºäº†ELKæ—¥å¿—åˆ†æç³»ç»Ÿçš„å®Œæ•´åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. ç»“æ„åŒ–æ—¥å¿—è®°å½•
2. æ—¥å¿—æ¨¡å¼åŒ¹é…å’Œå¼‚å¸¸æ£€æµ‹
3. æ—¥å¿—èšåˆå’Œç»Ÿè®¡åˆ†æ
4. æ€§èƒ½ç›‘æ§é›†æˆ
5. ä»ªè¡¨æ¿æ•°æ®ç”Ÿæˆ
6. æ—¥å¿—æœç´¢å’ŒæŸ¥è¯¢
"""

import asyncio
import time
import threading
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any

from stock_analysis_system.monitoring.elk_logging import (
    ELKLogger, LogLevel, LogCategory,
    initialize_elk_logging, get_elk_logger,
    log_info, log_warning, log_error, log_performance
)


class ELKLoggingDemo:
    """ELKæ—¥å¿—ç³»ç»Ÿæ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.logger = None
        self.demo_running = False
    
    def initialize_system(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        print("ğŸš€ åˆå§‹åŒ–ELKæ—¥å¿—åˆ†æç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–å…¨å±€æ—¥å¿—ç³»ç»Ÿ
        self.logger = initialize_elk_logging(
            elasticsearch_hosts=["localhost:9200"],  # å¦‚æœæœ‰ESæœåŠ¡å™¨
            index_prefix="stock-analysis-demo"
        )
        
        print(f"âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   - Elasticsearchå¯ç”¨: {self.logger.es_available}")
        print(f"   - ç´¢å¼•å‰ç¼€: {self.logger.index_prefix}")
        print(f"   - ç¼“å†²åŒºå¤§å°: {self.logger.buffer_size}")
        print()
    
    def demonstrate_basic_logging(self):
        """æ¼”ç¤ºåŸºæœ¬æ—¥å¿—è®°å½•åŠŸèƒ½"""
        print("ğŸ“ æ¼”ç¤ºåŸºæœ¬æ—¥å¿—è®°å½•åŠŸèƒ½...")
        
        # ä½¿ç”¨ä¸åŒçº§åˆ«è®°å½•æ—¥å¿—
        log_info("ç³»ç»Ÿå¯åŠ¨å®Œæˆ", "system", version="1.0.0", startup_time=2.5)
        log_info("ç”¨æˆ·ç™»å½•æˆåŠŸ", "auth", user_id="user123", ip="192.168.1.100")
        log_warning("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜", "monitor", memory_usage=85.5, threshold=80.0)
        log_error("æ•°æ®åº“è¿æ¥å¤±è´¥", "database", error_code="DB001", retry_count=3)
        log_performance("APIè¯·æ±‚å¤„ç†å®Œæˆ", "api", 150.5, endpoint="/api/stocks", method="GET")
        
        # ç›´æ¥ä½¿ç”¨loggerè®°å½•æ›´å¤æ‚çš„æ—¥å¿—
        self.logger.log(
            level=LogLevel.INFO,
            category=LogCategory.BUSINESS,
            message="è‚¡ç¥¨æ•°æ®æ›´æ–°å®Œæˆ",
            component="data_processor",
            user_id="system",
            metadata={
                "stocks_updated": 1500,
                "update_duration": 45.2,
                "data_source": "eastmoney",
                "success_rate": 98.5
            }
        )
        
        print(f"âœ… å·²è®°å½• {len(self.logger.log_buffer)} æ¡æ—¥å¿—")
        print()
    
    def demonstrate_pattern_matching(self):
        """æ¼”ç¤ºæ—¥å¿—æ¨¡å¼åŒ¹é…å’Œå¼‚å¸¸æ£€æµ‹"""
        print("ğŸ” æ¼”ç¤ºæ—¥å¿—æ¨¡å¼åŒ¹é…å’Œå¼‚å¸¸æ£€æµ‹...")
        
        # æ¨¡æ‹Ÿå„ç§é”™è¯¯æ¨¡å¼
        error_scenarios = [
            ("æ•°æ®åº“è¿æ¥è¶…æ—¶", "database", "Database connection timeout after 30 seconds"),
            ("APIè¯·æ±‚è¶…æ—¶", "api", "API request timeout - external service unavailable"),
            ("å†…å­˜ä¸è¶³è­¦å‘Š", "system", "Memory usage warning - 95% of available memory used"),
            ("ç”¨æˆ·è®¤è¯å¤±è´¥", "auth", "Authentication failed - invalid credentials provided"),
            ("æ•°æ®éªŒè¯é”™è¯¯", "validator", "Data validation error - invalid stock symbol format"),
            ("æ•°æ®åº“è¿æ¥é”™è¯¯", "database", "Database connection error - host unreachable"),
            ("APIè¶…æ—¶å¼‚å¸¸", "api", "Timeout occurred while calling external API")
        ]
        
        for component, category, message in error_scenarios:
            self.logger.log(
                level=LogLevel.ERROR,
                category=LogCategory.ERROR,
                message=message,
                component=component,
                timestamp_override=datetime.now()
            )
        
        # æ£€æŸ¥æ£€æµ‹åˆ°çš„å¼‚å¸¸
        anomalies = self.logger.anomaly_detector.get_recent_anomalies(hours=1)
        
        print(f"âœ… æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸æ¨¡å¼:")
        for anomaly in anomalies:
            print(f"   - {anomaly.pattern_name}: {anomaly.message[:50]}...")
        print()
    
    def demonstrate_log_aggregation(self):
        """æ¼”ç¤ºæ—¥å¿—èšåˆå’Œç»Ÿè®¡åˆ†æ"""
        print("ğŸ“Š æ¼”ç¤ºæ—¥å¿—èšåˆå’Œç»Ÿè®¡åˆ†æ...")
        
        # ç”Ÿæˆå¤§é‡æ—¥å¿—æ•°æ®è¿›è¡Œèšåˆ
        components = ["api", "database", "cache", "processor", "monitor"]
        categories = [LogCategory.SYSTEM, LogCategory.API, LogCategory.PERFORMANCE, LogCategory.DATA_ACCESS]
        levels = [LogLevel.INFO, LogLevel.WARNING, LogLevel.ERROR]
        
        for i in range(50):
            component = random.choice(components)
            category = random.choice(categories)
            level = random.choice(levels)
            
            # æ ¹æ®çº§åˆ«ç”Ÿæˆä¸åŒçš„æ¶ˆæ¯
            if level == LogLevel.INFO:
                message = f"{component} æ“ä½œå®Œæˆ"
            elif level == LogLevel.WARNING:
                message = f"{component} æ€§èƒ½è­¦å‘Š"
            else:
                message = f"{component} æ“ä½œå¤±è´¥"
            
            self.logger.log(
                level=level,
                category=category,
                message=message,
                component=component,
                duration_ms=random.uniform(50, 500),
                metadata={"operation_id": f"op_{i}"}
            )
        
        # è·å–èšåˆç»Ÿè®¡
        stats = self.logger.get_log_statistics(hours=1)
        
        print("âœ… æ—¥å¿—èšåˆç»Ÿè®¡:")
        print(f"   - æ—¥å¿—çº§åˆ«åˆ†å¸ƒ: {stats['aggregated_stats']['log_counts']}")
        print(f"   - é”™è¯¯æ¨¡å¼æ•°é‡: {len(stats['aggregated_stats']['error_patterns'])}")
        print(f"   - æ€§èƒ½æŒ‡æ ‡ç»„ä»¶: {list(stats['aggregated_stats']['performance_summary'].keys())}")
        print(f"   - æ£€æµ‹åˆ°çš„å¼‚å¸¸: {stats['total_anomalies']}")
        print()
    
    def demonstrate_performance_monitoring(self):
        """æ¼”ç¤ºæ€§èƒ½ç›‘æ§é›†æˆ"""
        print("âš¡ æ¼”ç¤ºæ€§èƒ½ç›‘æ§é›†æˆ...")
        
        # æ¨¡æ‹Ÿä¸åŒç»„ä»¶çš„æ€§èƒ½æ•°æ®
        performance_scenarios = [
            ("è‚¡ç¥¨æ•°æ®è·å–", "data_fetcher", 120.5),
            ("æŠ€æœ¯æŒ‡æ ‡è®¡ç®—", "indicator_calculator", 85.2),
            ("é£é™©è¯„ä¼°", "risk_assessor", 200.8),
            ("æ•°æ®åº“æŸ¥è¯¢", "database", 45.3),
            ("ç¼“å­˜æ“ä½œ", "cache", 15.7),
            ("APIå“åº”", "api", 180.4),
            ("æ•°æ®éªŒè¯", "validator", 35.9),
            ("æŠ¥å‘Šç”Ÿæˆ", "report_generator", 350.2)
        ]
        
        for operation, component, duration in performance_scenarios:
            log_performance(
                f"{operation}å®Œæˆ",
                component,
                duration,
                operation=operation.lower().replace(" ", "_"),
                success=True
            )
        
        # æ¨¡æ‹Ÿå¼‚å¸¸æ€§èƒ½æƒ…å†µ
        log_performance(
            "æ•°æ®åº“æŸ¥è¯¢è¶…æ—¶",
            "database",
            5000.0,  # å¼‚å¸¸é«˜çš„å“åº”æ—¶é—´
            operation="slow_query",
            success=False,
            query="SELECT * FROM large_table"
        )
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = self.logger.get_log_statistics()
        perf_summary = stats['aggregated_stats']['performance_summary']
        
        print("âœ… æ€§èƒ½ç›‘æ§ç»Ÿè®¡:")
        for component, metrics in perf_summary.items():
            print(f"   - {component}:")
            print(f"     å¹³å‡å“åº”æ—¶é—´: {metrics['avg_duration']:.2f}ms")
            print(f"     æœ€å¤§å“åº”æ—¶é—´: {metrics['max_duration']:.2f}ms")
            print(f"     æ“ä½œæ¬¡æ•°: {metrics['count']}")
        print()
    
    def demonstrate_dashboard_data(self):
        """æ¼”ç¤ºä»ªè¡¨æ¿æ•°æ®ç”Ÿæˆ"""
        print("ğŸ“ˆ æ¼”ç¤ºä»ªè¡¨æ¿æ•°æ®ç”Ÿæˆ...")
        
        dashboard_data = self.logger.create_dashboard_data()
        
        print("âœ… ä»ªè¡¨æ¿æ•°æ®:")
        print(f"   - ç³»ç»Ÿå¥åº·çŠ¶æ€: {dashboard_data['health_status']}")
        print(f"   - æ—¥å¿—çº§åˆ«åˆ†å¸ƒ: {dashboard_data['log_levels']}")
        print(f"   - é”™è¯¯æ¨¡å¼æ•°é‡: {len(dashboard_data['error_patterns'])}")
        print(f"   - æ€§èƒ½ç›‘æ§ç»„ä»¶: {len(dashboard_data['performance_metrics'])}")
        print(f"   - å¼‚å¸¸æ•°é‡: {len(dashboard_data['anomalies'])}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªé”™è¯¯æ¨¡å¼
        if dashboard_data['error_patterns']:
            print("   - ä¸»è¦é”™è¯¯æ¨¡å¼:")
            for pattern, count in list(dashboard_data['error_patterns'].items())[:3]:
                print(f"     {pattern}: {count}æ¬¡")
        
        # æ˜¾ç¤ºæœ€è¿‘çš„å¼‚å¸¸
        if dashboard_data['anomalies']:
            print("   - æœ€è¿‘å¼‚å¸¸:")
            for anomaly in dashboard_data['anomalies'][:3]:
                print(f"     {anomaly['pattern_name']}: {anomaly['message'][:40]}...")
        print()
    
    def demonstrate_log_search(self):
        """æ¼”ç¤ºæ—¥å¿—æœç´¢åŠŸèƒ½"""
        print("ğŸ” æ¼”ç¤ºæ—¥å¿—æœç´¢åŠŸèƒ½...")
        
        # å¦‚æœElasticsearchå¯ç”¨ï¼Œæ¼”ç¤ºæœç´¢åŠŸèƒ½
        if self.logger.es_available:
            print("âœ… Elasticsearchå¯ç”¨ï¼Œæ¼”ç¤ºæœç´¢åŠŸèƒ½:")
            
            # æœç´¢é”™è¯¯æ—¥å¿—
            error_logs = self.logger.search_logs(
                query="error",
                level=LogLevel.ERROR,
                size=5
            )
            print(f"   - æ‰¾åˆ° {len(error_logs)} æ¡é”™è¯¯æ—¥å¿—")
            
            # æœç´¢ç‰¹å®šç»„ä»¶çš„æ—¥å¿—
            api_logs = self.logger.search_logs(
                component="api",
                size=5
            )
            print(f"   - æ‰¾åˆ° {len(api_logs)} æ¡APIç»„ä»¶æ—¥å¿—")
            
            # æ—¶é—´èŒƒå›´æœç´¢
            recent_logs = self.logger.search_logs(
                start_time=datetime.now() - timedelta(hours=1),
                size=10
            )
            print(f"   - æ‰¾åˆ° {len(recent_logs)} æ¡æœ€è¿‘1å°æ—¶çš„æ—¥å¿—")
        else:
            print("âš ï¸  Elasticsearchä¸å¯ç”¨ï¼Œè·³è¿‡æœç´¢æ¼”ç¤º")
            print("   - åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¯ä»¥ä½¿ç”¨Elasticsearchè¿›è¡Œé«˜æ•ˆçš„æ—¥å¿—æœç´¢")
            print("   - æ”¯æŒå…¨æ–‡æœç´¢ã€æ—¶é—´èŒƒå›´æŸ¥è¯¢ã€å­—æ®µè¿‡æ»¤ç­‰åŠŸèƒ½")
        print()
    
    def demonstrate_concurrent_logging(self):
        """æ¼”ç¤ºå¹¶å‘æ—¥å¿—è®°å½•"""
        print("ğŸ”„ æ¼”ç¤ºå¹¶å‘æ—¥å¿—è®°å½•...")
        
        def worker_thread(worker_id: int, log_count: int):
            """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
            for i in range(log_count):
                # éšæœºé€‰æ‹©æ—¥å¿—çº§åˆ«å’Œç»„ä»¶
                level = random.choice([LogLevel.INFO, LogLevel.WARNING, LogLevel.ERROR])
                component = f"worker_{worker_id}"
                
                self.logger.log(
                    level=level,
                    category=LogCategory.SYSTEM,
                    message=f"Worker {worker_id} å¤„ç†ä»»åŠ¡ {i}",
                    component=component,
                    worker_id=worker_id,
                    task_id=i,
                    duration_ms=random.uniform(10, 100)
                )
                
                # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                time.sleep(0.01)
        
        # å¯åŠ¨å¤šä¸ªå·¥ä½œçº¿ç¨‹
        threads = []
        worker_count = 5
        logs_per_worker = 10
        
        start_time = time.time()
        
        for worker_id in range(worker_count):
            thread = threading.Thread(
                target=worker_thread,
                args=(worker_id, logs_per_worker)
            )
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        end_time = time.time()
        
        print(f"âœ… å¹¶å‘æ—¥å¿—è®°å½•å®Œæˆ:")
        print(f"   - å·¥ä½œçº¿ç¨‹æ•°: {worker_count}")
        print(f"   - æ¯çº¿ç¨‹æ—¥å¿—æ•°: {logs_per_worker}")
        print(f"   - æ€»æ—¥å¿—æ•°: {worker_count * logs_per_worker}")
        print(f"   - å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
        
        # éªŒè¯æ—¥å¿—å®Œæ•´æ€§
        stats = self.logger.get_log_statistics()
        total_logs = sum(stats['aggregated_stats']['log_counts'].values())
        print(f"   - ç»Ÿè®¡ä¸­çš„æ—¥å¿—æ€»æ•°: {total_logs}")
        print()
    
    def demonstrate_anomaly_detection(self):
        """æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹åŠŸèƒ½"""
        print("ğŸš¨ æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹åŠŸèƒ½...")
        
        # å»ºç«‹æ­£å¸¸çš„åŸºçº¿æ•°æ®
        print("   å»ºç«‹æ€§èƒ½åŸºçº¿...")
        for i in range(20):
            self.logger.anomaly_detector.update_baseline(
                "api_service", "response_time", 100.0 + random.uniform(-10, 10)
            )
        
        # æµ‹è¯•æ­£å¸¸å€¼
        normal_value = 105.0
        is_anomaly = self.logger.anomaly_detector.detect_anomaly(
            "api_service", "response_time", normal_value
        )
        print(f"   æ­£å¸¸å€¼ {normal_value}ms æ˜¯å¦å¼‚å¸¸: {is_anomaly}")
        
        # æµ‹è¯•å¼‚å¸¸å€¼
        anomaly_value = 500.0
        is_anomaly = self.logger.anomaly_detector.detect_anomaly(
            "api_service", "response_time", anomaly_value
        )
        print(f"   å¼‚å¸¸å€¼ {anomaly_value}ms æ˜¯å¦å¼‚å¸¸: {is_anomaly}")
        
        if is_anomaly:
            # è®°å½•å¼‚å¸¸æ—¥å¿—
            log_error(
                f"APIå“åº”æ—¶é—´å¼‚å¸¸: {anomaly_value}ms",
                "api_service",
                response_time=anomaly_value,
                threshold="åŸºçº¿+3Ïƒ"
            )
        
        print("âœ… å¼‚å¸¸æ£€æµ‹æ¼”ç¤ºå®Œæˆ")
        print()
    
    def demonstrate_log_lifecycle(self):
        """æ¼”ç¤ºæ—¥å¿—ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
        print("ğŸ”„ æ¼”ç¤ºæ—¥å¿—ç”Ÿå‘½å‘¨æœŸç®¡ç†...")
        
        # è®°å½•ç³»ç»Ÿå¯åŠ¨æ—¥å¿—
        log_info("ç³»ç»Ÿå¯åŠ¨", "system", phase="startup")
        
        # è®°å½•ä¸šåŠ¡æ“ä½œæ—¥å¿—
        log_info("å¼€å§‹æ•°æ®å¤„ç†", "processor", batch_id="batch_001")
        log_performance("æ•°æ®å¤„ç†å®Œæˆ", "processor", 1250.5, batch_id="batch_001", records=1000)
        
        # è®°å½•å¼‚å¸¸å’Œæ¢å¤
        log_error("æ•°æ®æºè¿æ¥å¤±è´¥", "data_source", source="eastmoney", error="timeout")
        log_info("åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº", "data_source", source="backup", action="failover")
        log_info("æ•°æ®æºè¿æ¥æ¢å¤", "data_source", source="eastmoney", action="recovery")
        
        # è®°å½•ç³»ç»Ÿå…³é—­æ—¥å¿—
        log_info("ç³»ç»Ÿå‡†å¤‡å…³é—­", "system", phase="shutdown")
        
        print("âœ… æ—¥å¿—ç”Ÿå‘½å‘¨æœŸæ¼”ç¤ºå®Œæˆ")
        print(f"   - å½“å‰ç¼“å†²åŒºæ—¥å¿—æ•°: {len(self.logger.log_buffer)}")
        print()
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print("ğŸ“‹ ç”ŸæˆELKæ—¥å¿—ç³»ç»Ÿæ¼”ç¤ºæ€»ç»“æŠ¥å‘Š...")
        
        # è·å–æœ€ç»ˆç»Ÿè®¡
        stats = self.logger.get_log_statistics()
        dashboard_data = self.logger.create_dashboard_data()
        
        print("\n" + "="*60)
        print("ELKæ—¥å¿—åˆ†æç³»ç»Ÿæ¼”ç¤ºæ€»ç»“æŠ¥å‘Š")
        print("="*60)
        
        print(f"\nğŸ“Š ç³»ç»Ÿæ¦‚è§ˆ:")
        print(f"   ç³»ç»Ÿå¥åº·çŠ¶æ€: {dashboard_data['health_status']}")
        print(f"   ElasticsearchçŠ¶æ€: {'å¯ç”¨' if self.logger.es_available else 'ä¸å¯ç”¨'}")
        print(f"   æ—¥å¿—ç´¢å¼•å‰ç¼€: {self.logger.index_prefix}")
        
        print(f"\nğŸ“ˆ æ—¥å¿—ç»Ÿè®¡:")
        log_counts = stats['aggregated_stats']['log_counts']
        total_logs = sum(log_counts.values())
        print(f"   æ€»æ—¥å¿—æ•°: {total_logs}")
        for level, count in log_counts.items():
            percentage = (count / total_logs * 100) if total_logs > 0 else 0
            print(f"   {level}: {count} ({percentage:.1f}%)")
        
        print(f"\nğŸš¨ å¼‚å¸¸æ£€æµ‹:")
        print(f"   æ£€æµ‹åˆ°çš„å¼‚å¸¸æ€»æ•°: {stats['total_anomalies']}")
        print(f"   é”™è¯¯æ¨¡å¼æ•°é‡: {len(stats['aggregated_stats']['error_patterns'])}")
        
        if stats['recent_anomalies']:
            print(f"   æœ€è¿‘å¼‚å¸¸:")
            for anomaly in stats['recent_anomalies'][:3]:
                print(f"     - {anomaly['pattern_name']}: {anomaly['severity']}")
        
        print(f"\nâš¡ æ€§èƒ½ç›‘æ§:")
        perf_summary = stats['aggregated_stats']['performance_summary']
        print(f"   ç›‘æ§ç»„ä»¶æ•°: {len(perf_summary)}")
        
        if perf_summary:
            print(f"   æ€§èƒ½æ¦‚è§ˆ:")
            for component, metrics in perf_summary.items():
                print(f"     {component}: å¹³å‡{metrics['avg_duration']:.1f}ms "
                      f"(æœ€å¤§{metrics['max_duration']:.1f}ms, {metrics['count']}æ¬¡)")
        
        print(f"\nğŸ”§ ç³»ç»Ÿç‰¹æ€§:")
        print(f"   âœ… ç»“æ„åŒ–æ—¥å¿—è®°å½•")
        print(f"   âœ… å®æ—¶æ¨¡å¼åŒ¹é…å’Œå¼‚å¸¸æ£€æµ‹")
        print(f"   âœ… æ—¥å¿—èšåˆå’Œç»Ÿè®¡åˆ†æ")
        print(f"   âœ… æ€§èƒ½ç›‘æ§é›†æˆ")
        print(f"   âœ… å¹¶å‘å®‰å…¨çš„æ—¥å¿—å¤„ç†")
        print(f"   âœ… ä»ªè¡¨æ¿æ•°æ®ç”Ÿæˆ")
        print(f"   {'âœ…' if self.logger.es_available else 'âš ï¸ '} Elasticsearché›†æˆ")
        
        print(f"\nğŸ’¡ ç”Ÿäº§ç¯å¢ƒå»ºè®®:")
        print(f"   - é…ç½®Elasticsearché›†ç¾¤ä»¥æ”¯æŒå¤§è§„æ¨¡æ—¥å¿—å­˜å‚¨")
        print(f"   - è®¾ç½®Kibanaä»ªè¡¨æ¿è¿›è¡Œå¯è§†åŒ–åˆ†æ")
        print(f"   - é…ç½®Logstashè¿›è¡Œæ—¥å¿—é¢„å¤„ç†å’Œè½¬æ¢")
        print(f"   - å®æ–½æ—¥å¿—è½®è½¬å’Œå½’æ¡£ç­–ç•¥")
        print(f"   - è®¾ç½®å®æ—¶å‘Šè­¦å’Œé€šçŸ¥æœºåˆ¶")
        
        print("\n" + "="*60)
        print("æ¼”ç¤ºå®Œæˆï¼ELKæ—¥å¿—åˆ†æç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªã€‚")
        print("="*60)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.logger:
            print("\nğŸ§¹ æ¸…ç†ç³»ç»Ÿèµ„æº...")
            self.logger.shutdown()
            print("âœ… ç³»ç»Ÿèµ„æºæ¸…ç†å®Œæˆ")
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        try:
            print("ğŸ¯ å¼€å§‹ELKæ—¥å¿—åˆ†æç³»ç»Ÿå®Œæ•´æ¼”ç¤º")
            print("="*60)
            
            # åˆå§‹åŒ–ç³»ç»Ÿ
            self.initialize_system()
            
            # æ¼”ç¤ºå„ä¸ªåŠŸèƒ½
            self.demonstrate_basic_logging()
            self.demonstrate_pattern_matching()
            self.demonstrate_log_aggregation()
            self.demonstrate_performance_monitoring()
            self.demonstrate_dashboard_data()
            self.demonstrate_log_search()
            self.demonstrate_concurrent_logging()
            self.demonstrate_anomaly_detection()
            self.demonstrate_log_lifecycle()
            
            # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            self.generate_summary_report()
            
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # æ¸…ç†èµ„æº
            self.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    demo = ELKLoggingDemo()
    demo.run_complete_demo()


if __name__ == "__main__":
    main()
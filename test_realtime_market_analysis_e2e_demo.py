#!/usr/bin/env python3
"""
å®æ—¶è¡Œæƒ…åˆ†æç«¯åˆ°ç«¯æµ‹è¯•æ¼”ç¤º

æ¼”ç¤ºå®Œæ•´çš„å®æ—¶è¡Œæƒ…åˆ†ææµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®è·å–ã€å¤„ç†ã€å­˜å‚¨ã€åˆ†æå’ŒAPIå“åº”çš„ç«¯åˆ°ç«¯æµ‹è¯•ã€‚
"""

import asyncio
import sys
import os
import logging
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from tests.test_realtime_market_analysis_e2e import RealtimeMarketAnalysisE2ETest

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('realtime_e2e_test.log')
    ]
)
logger = logging.getLogger(__name__)


async def run_comprehensive_e2e_test():
    """è¿è¡Œç»¼åˆç«¯åˆ°ç«¯æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹å®æ—¶è¡Œæƒ…åˆ†æç«¯åˆ°ç«¯æµ‹è¯•æ¼”ç¤º")
    print("=" * 80)
    
    test_suite = RealtimeMarketAnalysisE2ETest()
    
    try:
        # 1. è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        print("\nğŸ“‹ æ­¥éª¤ 1: è®¾ç½®æµ‹è¯•ç¯å¢ƒ")
        print("-" * 40)
        await test_suite.setup_test_environment()
        print("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
        
        # 2. æ‰§è¡Œå®Œæ•´æµç¨‹æµ‹è¯•
        print("\nğŸ”„ æ­¥éª¤ 2: æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹æµ‹è¯•")
        print("-" * 40)
        
        flow_results = await test_suite.test_complete_realtime_analysis_flow()
        
        # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
        print("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        performance = flow_results.get('performance_metrics', {})
        for metric, data in performance.items():
            if isinstance(data, dict) and 'duration' in data:
                status = "âœ…" if data.get('meets_threshold', False) else "âš ï¸"
                print(f"  {status} {metric}: {data['duration']:.2f}ç§’")
        
        total_time = performance.get('total_duration', 0)
        sla_status = "âœ…" if performance.get('meets_sla', False) else "âš ï¸"
        print(f"  {sla_status} æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        # æ˜¾ç¤ºæ•°æ®è´¨é‡æŒ‡æ ‡
        print("\nğŸ“ˆ æ•°æ®è´¨é‡æŒ‡æ ‡:")
        quality = flow_results.get('data_quality_metrics', {})
        print(f"  å®Œæ•´æ€§è¯„åˆ†: {quality.get('completeness_score', 0):.2f}")
        print(f"  å‡†ç¡®æ€§è¯„åˆ†: {quality.get('accuracy_score', 0):.2f}")
        print(f"  æ—¶æ•ˆæ€§è¯„åˆ†: {quality.get('timeliness_score', 0):.2f}")
        print(f"  ä¸€è‡´æ€§è¯„åˆ†: {quality.get('consistency_score', 0):.2f}")
        print(f"  æ€»ä½“è¯„åˆ†: {quality.get('overall_score', 0):.2f}")
        
        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        if flow_results.get('error_details'):
            print("\nâŒ é”™è¯¯è¯¦æƒ…:")
            for error in flow_results['error_details']:
                print(f"  â€¢ {error}")
        
        # 3. æ‰§è¡Œå¼‚å¸¸æ¢å¤æµ‹è¯•
        print("\nğŸ›¡ï¸ æ­¥éª¤ 3: æ‰§è¡Œå¼‚å¸¸æ¢å¤æµ‹è¯•")
        print("-" * 40)
        
        recovery_results = await test_suite.test_exception_recovery()
        
        print("å¼‚å¸¸æ¢å¤èƒ½åŠ›æµ‹è¯•ç»“æœ:")
        recovery_tests = [
            ('ç½‘ç»œå¼‚å¸¸æ¢å¤', recovery_results.get('network_failure_recovery', False)),
            ('æ•°æ®æºå¼‚å¸¸æ¢å¤', recovery_results.get('data_source_failure_recovery', False)),
            ('æ•°æ®åº“å¼‚å¸¸æ¢å¤', recovery_results.get('database_failure_recovery', False)),
            ('ç¼“å­˜å¼‚å¸¸æ¢å¤', recovery_results.get('cache_failure_recovery', False))
        ]
        
        for test_name, passed in recovery_tests:
            status = "âœ…" if passed else "âŒ"
            print(f"  {status} {test_name}")
        
        # 4. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        print("\nğŸ“„ æ­¥éª¤ 4: ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
        print("-" * 40)
        
        report = generate_test_report(flow_results, recovery_results)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_filename = f"realtime_e2e_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")
        
        # 5. æ€»ç»“
        print("\nğŸ¯ æµ‹è¯•æ€»ç»“")
        print("-" * 40)
        
        overall_success = flow_results.get('success', False) and all(recovery_results.values())
        status_icon = "ğŸ‰" if overall_success else "âš ï¸"
        status_text = "å…¨éƒ¨é€šè¿‡" if overall_success else "éƒ¨åˆ†å¤±è´¥"
        
        print(f"{status_icon} ç«¯åˆ°ç«¯æµ‹è¯•ç»“æœ: {status_text}")
        print(f"ğŸ“Š æ•°æ®è´¨é‡è¯„åˆ†: {quality.get('overall_score', 0):.2f}/1.00")
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        print(f"ğŸ”§ å¼‚å¸¸æ¢å¤èƒ½åŠ›: {sum(recovery_results.values())}/{len(recovery_results)} é¡¹é€šè¿‡")
        
        return overall_success
        
    except Exception as e:
        logger.error(f"ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return False
        
    finally:
        # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        print("\nğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
        await test_suite.teardown_test_environment()
        print("âœ… ç¯å¢ƒæ¸…ç†å®Œæˆ")


def generate_test_report(flow_results, recovery_results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report = {
        'test_info': {
            'test_name': 'å®æ—¶è¡Œæƒ…åˆ†æç«¯åˆ°ç«¯æµ‹è¯•',
            'test_time': datetime.now().isoformat(),
            'test_version': '1.0.0'
        },
        'flow_test_results': flow_results,
        'recovery_test_results': recovery_results,
        'summary': {
            'overall_success': flow_results.get('success', False) and all(recovery_results.values()),
            'total_duration': flow_results.get('performance_metrics', {}).get('total_duration', 0),
            'data_quality_score': flow_results.get('data_quality_metrics', {}).get('overall_score', 0),
            'recovery_success_rate': sum(recovery_results.values()) / len(recovery_results) if recovery_results else 0
        },
        'recommendations': []
    }
    
    # ç”Ÿæˆå»ºè®®
    quality_score = report['summary']['data_quality_score']
    if quality_score < 0.8:
        report['recommendations'].append("æ•°æ®è´¨é‡è¯„åˆ†è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æºå’ŒéªŒè¯è§„åˆ™")
    
    total_time = report['summary']['total_duration']
    if total_time > 5.0:
        report['recommendations'].append("å“åº”æ—¶é—´è¶…è¿‡SLAè¦æ±‚ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½")
    
    recovery_rate = report['summary']['recovery_success_rate']
    if recovery_rate < 1.0:
        report['recommendations'].append("éƒ¨åˆ†å¼‚å¸¸æ¢å¤æµ‹è¯•å¤±è´¥ï¼Œå»ºè®®åŠ å¼ºå®¹é”™æœºåˆ¶")
    
    return report


async def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nğŸƒâ€â™‚ï¸ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    test_suite = RealtimeMarketAnalysisE2ETest()
    
    try:
        await test_suite.setup_test_environment()
        
        # å¤šæ¬¡è¿è¡Œæµ‹è¯•ä»¥è·å¾—å¹³å‡æ€§èƒ½
        iterations = 3
        total_times = []
        
        for i in range(iterations):
            print(f"\nç¬¬ {i+1}/{iterations} æ¬¡æµ‹è¯•...")
            
            results = await test_suite.test_complete_realtime_analysis_flow()
            total_time = results.get('performance_metrics', {}).get('total_duration', 0)
            total_times.append(total_time)
            
            print(f"æœ¬æ¬¡è€—æ—¶: {total_time:.2f}ç§’")
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        avg_time = sum(total_times) / len(total_times)
        min_time = min(total_times)
        max_time = max(total_times)
        
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’")
        print(f"  æœ€çŸ­è€—æ—¶: {min_time:.2f}ç§’")
        print(f"  æœ€é•¿è€—æ—¶: {max_time:.2f}ç§’")
        print(f"  æ€§èƒ½ç¨³å®šæ€§: {'è‰¯å¥½' if (max_time - min_time) < 1.0 else 'ä¸€èˆ¬'}")
        
    finally:
        await test_suite.teardown_test_environment()


async def run_stress_test():
    """è¿è¡Œå‹åŠ›æµ‹è¯•"""
    print("\nğŸ’ª å‹åŠ›æµ‹è¯•")
    print("=" * 50)
    
    test_suite = RealtimeMarketAnalysisE2ETest()
    
    try:
        await test_suite.setup_test_environment()
        
        # å¢åŠ æµ‹è¯•è‚¡ç¥¨æ•°é‡è¿›è¡Œå‹åŠ›æµ‹è¯•
        original_symbols = test_suite.test_symbols
        stress_symbols = original_symbols * 3  # å¢åŠ åˆ°3å€
        test_suite.test_symbols = stress_symbols
        
        print(f"å‹åŠ›æµ‹è¯•è‚¡ç¥¨æ•°é‡: {len(stress_symbols)}")
        
        start_time = asyncio.get_event_loop().time()
        results = await test_suite.test_complete_realtime_analysis_flow()
        end_time = asyncio.get_event_loop().time()
        
        total_time = end_time - start_time
        throughput = len(stress_symbols) / total_time if total_time > 0 else 0
        
        print(f"å‹åŠ›æµ‹è¯•ç»“æœ:")
        print(f"  å¤„ç†è‚¡ç¥¨æ•°é‡: {len(stress_symbols)}")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"  å¤„ç†ååé‡: {throughput:.2f} è‚¡ç¥¨/ç§’")
        print(f"  æµ‹è¯•ç»“æœ: {'é€šè¿‡' if results.get('success', False) else 'å¤±è´¥'}")
        
        # æ¢å¤åŸå§‹è®¾ç½®
        test_suite.test_symbols = original_symbols
        
    finally:
        await test_suite.teardown_test_environment()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å®æ—¶è¡Œæƒ…åˆ†æç«¯åˆ°ç«¯æµ‹è¯•æ¼”ç¤ºç³»ç»Ÿ")
    print("=" * 80)
    
    try:
        # 1. ç»¼åˆç«¯åˆ°ç«¯æµ‹è¯•
        success = await run_comprehensive_e2e_test()
        
        if success:
            # 2. æ€§èƒ½åŸºå‡†æµ‹è¯•
            await run_performance_benchmark()
            
            # 3. å‹åŠ›æµ‹è¯•
            await run_stress_test()
        
        print("\n" + "=" * 80)
        print("ğŸ æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå‡ºç°å¼‚å¸¸: {e}")
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå‡ºç°å¼‚å¸¸: {e}")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())